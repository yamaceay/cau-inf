{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory and Graph Models\n",
    "\n",
    "Observational: $p((x_i)_i) = \\Pi_i p(x_i | PA_i)$ where $X_i \\perp\\!\\!\\!\\perp (X_{\\neq i} \\backslash PA_i) | PA_i$\n",
    "\n",
    "Different choice of Markovian parents results in different causal models => Quest for common separation relationships leads to $X \\bowtie Y | Z$\n",
    "\n",
    "Open -> Close: \n",
    "\n",
    "> In usual case, X and Y are correlated. If Z is intervened, they become disconnected. \n",
    "\n",
    "> For example $g(y := f(x)) = z$ or $f(y) = (x, z)$, and fixing $y$ to a constant means fixing $x$ and $z$ as well.  \n",
    "\n",
    "* Chain: $X \\to Z \\to Y$. \n",
    "* Fork: $Z \\to X, Y$\n",
    "\n",
    "Close -> Open: \n",
    "\n",
    "> X and Y have no common cause. If Z is intervened, the relationship of X and Y becomes dependent. \n",
    "\n",
    "> Take, for example $f(x, y) = z$. Fix $z$ to some constant, then you get the level set $N_{z = k} = \\{(x, y) | f(x, y) = z\\}$ where $x$ and $y$ have to depend on each other. \n",
    "\n",
    "* Collider (or n-th Desc): $X, Y \\to_n Z$\n",
    "\n",
    "Remember that the descendant relationship can go as far as possible, since $p(X_i | PA_i) \\ne 0$.\n",
    "\n",
    "D-separation is stricter than conditional independence ($X \\bowtie Y | Z \\implies X \\perp\\!\\!\\!\\perp Y | Z$)\n",
    "\n",
    "For any $X, Y$: D-separated if $\\forall Z . (matches(X, Y \\to_n Z) \\leftrightarrow Z \\notin S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCM\n",
    "\n",
    "$X_i := f_i(PA_i, \\eta_i) \\forall i$ with $PA_i \\subseteq X \\backslash X_i$\n",
    "\n",
    "* Observational distribution generated by $C$: $p_C((X_i)_i) = \\Pi_i p_C(X_i | PA_i)$ (**Observed**)\n",
    "* Interventional distribution generated by $C$: $p_C((X_i)_i | do(X_j := k_j)) = \\frac{p_C((X_i)_i)}{p_C(X_j | PA_j)} \\delta(X_j - k_j)$ if $p_C(X_j | PA_j) \\ne 0$ (**Unobserved**)\n",
    "\n",
    "Ladders of Causation:\n",
    "\n",
    "1. Association (Seeing): $p(Y | X)$, requires $p(\\cdot)$\n",
    "2. Intervention (Doing): $p(Y | do(X := x))$, requires $p(\\cdot)$ plus ${\\cal G}({\\cal C})$\n",
    "3. Counterfactuals (Thinking): $p(Y_X | Y'_{X'})$, requires $\\cal C$ ($\\implies {\\cal G}({\\cal C}), p(\\cdot)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Discovery\n",
    "\n",
    "* Constraint-based: Identify conditional independencies using statistical tests\n",
    "* Score-based: ML approach, minimize loss over predicting the observed data\n",
    "* SCM-based: Predict cause-effect relationships by analysing the noises\n",
    "\n",
    "Constraint-based\n",
    "\n",
    "Causal Markov Condition: ''d-separation implies independence'' (see Bayesian networks)\n",
    "\n",
    "1. Data Generation by SCM\n",
    "2. Acyclicity\n",
    "3. Causal Faithfulness: ''independence implies d-separation''\n",
    "4. Causal Sufficiency\n",
    "\n",
    "1-3: ''independence implies d-separation''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence Testing\n",
    "\n",
    "Hypothesis testing (frequentist): \n",
    "* Assume iid samples of random variables $X_i \\sim p_{X; \\theta}$\n",
    "* $p_{X; \\theta}$ depends on $\\theta$ with unknown value\n",
    "\n",
    "* Define a hypothesis $H_0: \\theta = \\theta_{0}$ for some $\\theta_{0}$\n",
    "* Choose test statistic $T$\n",
    "* Obtain null distribution $p(T | H_0)$\n",
    "* Fix a significance level $\\alpha \\in ]0, 1[$ for determining the rejection threshold\n",
    "* Collect relevant data $D_n$ consisting of n samples and calculate $T(data)$\n",
    "* Reject $H_0$ if $T(D_n)$ is outside region (or if p-value $\\le \\alpha$)\n",
    "\n",
    "Given power: $TN = 1 - \\beta$, high power depends on:\n",
    "* Effect size (dependency strength)\n",
    "* Significance $\\alpha$-level\n",
    "* [DoF] Degrees of freedom $\\sim$ sample size - number of parameters to estimate\n",
    "\n",
    "Given $m > 1$ tests:\n",
    "* $P(\\bigvee_i \\alpha_i) = 1 - P(\\bigwedge_i \\neg \\alpha_i) \\le 1 - P(\\bigwedge \\neg \\frac{\\alpha}{m}) = P(\\bigvee \\frac{\\alpha}{m}) = m * \\frac{\\alpha}{m} = \\alpha$\n",
    "\n",
    "Correlation Coefficients: \n",
    "\n",
    "* Pearson Correlation Coefficient:\n",
    "    * $\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X) Var(Y)}}$ for $(X_i, Y_i) \\overset{iid}{\\sim} p_{X, Y}$\n",
    "    * $X \\perp\\!\\!\\!\\perp Y \\implies \\rho(X, Y) = 0$ (or $\\iff$ if $X, Y$ jointly Gaussian)\n",
    "    * Estimate as:\n",
    "        * $\\hat{\\rho}(X, Y) = \\frac{\\hat{Cov}(X, Y)}{\\sqrt{\\hat{Var}(X) \\hat{Var}(Y)}}$\n",
    "        * $\\hat{Cov}(X, Y) = \\frac{1}{n}\\sum_{i = 1}^n (X_i - \\overline{X_n})(Y_i - \\overline{Y_n})$\n",
    "        * $\\hat{Var}(X) = \\frac{1}{n}\\sum_{i = 1}^n (X_i - \\overline{X_n})^2$\n",
    "        * $\\hat{Var}(Y) = \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\overline{Y_n})^2$\n",
    "\n",
    "* Distance Correlation Coefficient:\n",
    "    * $dCor(X, Y) = \\frac{dCov(X, Y)}{\\sqrt{dVar(X) dVar(Y)}}$ for $(X_i, Y_i) \\overset{iid}{\\sim} p_{X, Y}$\n",
    "    <!-- * $X \\perp\\!\\!\\!\\perp Y \\implies dCor(X, Y) = 0$ -->\n",
    "    * Estimate as:\n",
    "        * $dCor(X, Y) = \\frac{dCov(X, Y)}{\\sqrt{dVar(X) dVar(Y)}}$\n",
    "        * $dCov(r_X, r_Y) = \\frac{1}{n} \\sqrt{\\sum_{i, j} A_{ij} \\cdot B_{ij}}$\n",
    "        * $dVar(r_X) = \\frac{1}{n} \\sqrt{ \\sum_{i, j} A_{ij}^2}$\n",
    "        * $dVar(r_Y) = \\frac{1}{n} \\sqrt{ \\sum_{i, j} B_{ij}^2}$\n",
    "\n",
    "Linear Dependency Tests with Additive Noise: \n",
    "\n",
    "* Multivariate Gaussian:\n",
    "    * Partial Correlation Test (ParCorr)\n",
    "    * Null hypothesis: $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $X, Y, Z$ jointly Gaussian and $\\#(Z) = p$\n",
    "    * Assume $X = Z \\beta_X + \\epsilon_X, Y = Z \\beta_Y + \\epsilon_Y$ for all known common causes $Z$\n",
    "    * Predict $\\hat{\\beta}_X, \\hat{\\beta}_Y$ by minimizing OLS\n",
    "    * Residuals $r_X = X - Z \\hat{\\beta}_X, r_Y = Y - Z \\hat{\\beta}_Y$ \n",
    "    * Residuals are uncorrelated, also $\\rho(r_X, r_Y) = 0$ if $X \\perp\\!\\!\\!\\perp Y | Z$\n",
    "    * $t$-Test statistic: $T = \\sqrt{n - p - 2} \\frac{\\hat{\\rho}(r_X, r_Y)}{\\sqrt{1 - \\hat{\\rho}(r_X, r_Y)^2}}$ with $p$ as cardinality of $Z$\n",
    "    * $T \\sim T_{n - p - 2}$\n",
    "\n",
    "    * If $p = 0$: \n",
    "        * $r_X = \\epsilon_X + Z (\\beta_X - \\hat{\\beta}_X), r_Y = \\epsilon_Y + Z (\\beta_Y - \\hat{\\beta}_Y)$\n",
    "        * If $Z = 0$, residuals are $r_X = X, r_Y = Y$\n",
    "\n",
    "* Discrete:\n",
    "    * Null hypothesis: $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $X, Y, Z$ discrete and $\\#(Z) = p$\n",
    "    * $X \\perp\\!\\!\\!\\perp Y | Z \\iff p(X, Y | Z) = p(X | Z) p(Y | Z)$\n",
    "    * $\\Bbb{E}[S_{XYZ}^{abc} | S_{XZ}^{ac} = s_{XZ}^{ac}, S_{YZ}^{bc} = s_{YZ}^{bc}] = \\frac{s_{XZ}^{ac} \\cdot s_{YZ}^{bc}}{s_Z^{c}}$\n",
    "    * $\\chi^2$-Test statistic: $G^2 = 2\\sum_{abc} s_{XYZ}^{abc} \\log (\\frac{s_{XYZ}^{abc} \\cdot s_Z^c}{s_{XZ}^{ac} \\cdot s_{YZ}^{bc}})$\n",
    "    * $G^2 \\overset{n \\to \\infty}{\\sim} \\chi_v^2$ with DoF: $(m_X - 1)(m_Y - 1)\\max(p, 1)$\n",
    "    \n",
    "    * If $p = 0$: \n",
    "        * Replace $s_Z^c = 1$ and $\\sum_{abc} … = n \\sum_{ab}$\n",
    "        * Then $G^2 = 2\\sum_{ab} s_{XY}^{ab} \\log (\\frac{s_{XY}^{ab} \\cdot n}{s_{X}^{a} \\cdot s_{Y}^{b}})$\n",
    "        * DoF: $(m_X - 1)(m_Y - 1)$\n",
    "\n",
    "Non-linear Dependency Tests with Additive Noise:\n",
    "\n",
    "* Multivariate Gaussian:\n",
    "    * Gaussian process distance correlation test (GPDC)\n",
    "    * Null hypothesis $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $\\#(Z) = p$\n",
    "    * Assume $X = f_X(Z) + \\epsilon_X, Y = f_Y(Z) + \\epsilon_Y$ for all known common causes $Z$\n",
    "    * Residuals $r_X = X - \\hat{f_X}(Z), r_Y = Y - \\hat{f_Y}(Z)$\n",
    "    * $dCor(r_X, r_Y) = \\frac{dCov(r_X, r_Y)}{\\sqrt{dVar(r_X) dVar(r_Y)}}$\n",
    "    * Theoretical test distribution not known, use permutation testing for $dCor(r_X, r_Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Ideas\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "|                   | **Actual Positive (P)** | **Actual Negative (N)** |\n",
    "|-------------------|-------------------------|-------------------------|\n",
    "| **Predicted Positive (P')** | True Positive (TP): $1 - \\alpha$  | False Positive (FP): $\\beta$  |\n",
    "| **Predicted Negative (N')** | False Negative (FN): $\\alpha$  | True Negative (TN): $1 - \\beta$  |\n",
    "\n",
    "* Precision: $\\frac{TP}{TP + FP}$ - given all positive predictions, which are true positive\n",
    "* Recall (Sensitivity): $\\frac{TP}{TP + FN}$ - given all positive cases, which are predicted positive\n",
    "* Specificity: $\\frac{TN}{TN + FP}$ - given all negative cases, which are predicted negative\n",
    "\n",
    "* F1-Score: Harmonic mean of Precision and Recall - $\\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} = \\frac{2}{\\frac{TP + FP}{TP} + \\frac{TP + FN}{TP}} = \\frac{2}{\\frac{2 * TP + FP + FN}{TP}} = \\frac{2 * TP}{2 * TP + FP + FN} = \\frac{2 * (1 - \\alpha)}{2 * (1 - \\alpha) + \\alpha + \\beta} = \\frac{1 - \\alpha}{1 - (\\alpha + \\beta) / 2}$\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Degrees of Freedom:\n",
    "\n",
    "* If a LES has n equations and n parameters, exactly one solution\n",
    "* If a LES has n-1 equations, (linearly) infinitely many solutions (1 free variable)\n",
    "* If n-2 equations, (quadratically) infinitely many solutions (2 free variable)\n",
    "* … \n",
    "* More degrees of freedom, more multitudes of infinity\n",
    "* In an extreme case, A NN would have millions of parameters to estimate\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Bonferroni Correction:\n",
    "\n",
    "Given $m > 1$ tests:\n",
    "* Assume $\\alpha = \\Sigma_i \\alpha_i$\n",
    "* $P(\\bigvee_i \\alpha_i) = 1 - P(\\bigwedge_i \\neg \\alpha_i) = 1 - \\Pi_i P(\\alpha_i) = 1 - \\Pi_i (1 - \\alpha_i)$\n",
    "* $\\max \\frac{\\log P(\\bigvee_i \\alpha_i)}{\\Sigma_i \\alpha_i} = \\max \\frac{\\log (1 - \\Pi_i (1 - \\alpha_i))}{\\Sigma_i \\alpha_i} = \\min \\frac{\\log (\\Pi_i (1 - \\alpha_i))}{\\Sigma_i \\alpha_i} = \\min \\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}$\n",
    "* $0 = \\frac{\\partial}{\\partial \\alpha_i}(\\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}) = \\frac{\\partial}{\\partial \\alpha_i}(\\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}) = \\frac{\\frac{1}{\\alpha_i - 1}(\\Sigma_j \\alpha_j) - \\Sigma_j \\log(1 - \\alpha_j)}{(\\Sigma_j \\alpha_j)^2} \\sim \\frac{1}{\\alpha_i - 1}(\\Sigma_j \\alpha_j) - \\Sigma_j \\log(1 - \\alpha_j) = \\Sigma_j \\frac{\\alpha_j}{\\alpha_i - 1} - \\log(1 - \\alpha_j) = \\Sigma_j \\frac{\\alpha_j}{1 - \\alpha_i} - \\log(\\frac{1}{1 - \\alpha_j}) \\sim \\Sigma_j \\alpha_j - (1 - \\alpha_i) \\log(\\frac{1}{1 - \\alpha_j}) = \\alpha - (1 - \\alpha_i) \\Sigma_j \\log(\\frac{1}{1 - \\alpha_j}) \\sim \\alpha - (1 - \\alpha_i) k$ for some strictly positive constant $k$.\n",
    "\n",
    "Then for all $i$: $\\alpha = (1 - \\alpha_i)k \\iff \\alpha_i = 1 - \\frac{\\alpha}{k}$\n",
    "* Also, upper bound estimation yields $\\alpha_1 = … = \\alpha_m = \\frac{\\alpha}{m}$\n",
    "* $1 - \\Pi_i (1 - \\alpha_i) = 1 - \\Pi_i (1 - \\frac{\\alpha}{m}) \\le 1 - (1 - \\frac{\\alpha}{m})^m = 1 - P(\\bigwedge \\neg \\frac{\\alpha}{m}) = P(\\bigvee \\frac{\\alpha}{m}) = m * P(\\frac{\\alpha}{m}) = m * \\frac{\\alpha}{m} = \\alpha$\n",
    "* Interestingly, if $m \\to \\infty$, then $1 - (1 - \\frac{\\alpha}{m})^m \\to 1 - e^{-\\alpha}$\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Causal Sufficiency:\n",
    "\n",
    "* $p(w) = \\int p(w, l) dl$ with $L = X \\backslash W$ latent variables.\n",
    "* Implies there is no pair of distinct observed vertices with a hidden common cause (a latent variable).\n",
    "* All needed variables are within model, so the causal model is sufficient\n",
    "* Alternatively: $p((\\eta_i)_i) = \\Pi_i p(\\eta_i)$, also the noise independence\n",
    "\n",
    "<hr/>\n",
    "\n",
    "CMI (Conditional Mutual Independence):\n",
    "\n",
    "* $\\forall x, y, z$:\n",
    "    * $p(x, y | z) \\ge p(x | z) p(y | z)$\n",
    "    * $\\frac{p(x, y | z)}{p(x | z) p(y | z)} \\ge 1$\n",
    "    * $\\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} \\ge 0$\n",
    "* $\\int_z \\int_y \\int_x p(x, y, z) \\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} dx dy dz \\ge 0$\n",
    "* $\\int_z p(z) dz \\int_y \\int_x p(x, y | z) \\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} dx dy \\ge 0$\n",
    "* $I(X; Y | Z) \\ge 0$\n",
    "\n",
    "* If independent, make $\\ge$ to $=$.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Determinism:\n",
    "\n",
    "* $X = f(Z)$, then $X$ and $Z$ would have to be merged as a single node.\n",
    "* See Proof:\n",
    "    * $l(f(Z); Y | Z) = \\int_z p(z) dz \\int_y \\int_x p(f(z), y | z) \\log \\frac{p(f(z), y | z)}{p(f(z) | z) p(y | z)} dx dy$\n",
    "    * $p(f(z) | z) = 1$, because $X := f(z)$ is always satisfied if $Z := z$ \n",
    "    * $p(f(z), y | z) = p(y | z)$, because similarly $X := f(z)$ on the left side doesn't change anything.\n",
    "    * Then $p(f(z), y | z) \\log \\frac{p(f(z), y | z)}{p(f(z) | z) p(y | z)} = p(y | z) \\log \\frac{p(y | z)}{p(y | z)} = p(y | z) \\log 1 = 0$\n",
    "* Category Theory suggests that if two variables have the exactly same relationships with all other nodes, then they are the same. \n",
    "* Regard noises of variable as the characteristics of a person, and if a person is married to another to another, they are fully codependent and may be counted as one person.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevante Ideen\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Mehrfache Integration ist ein Funktional, die eine Funktion als Input nimmt und eine neue Funktion (oder eine Konstante) zurückgibt. \n",
    "\n",
    "Die unteren und oberen Grenzen der Integration müssen partiell geordnet sein, d.h. $\\int_a^b \\int_{\\alpha(x)}^{\\beta(x)} \\int_{\\rho(x, y)}^{\\sigma(x, y)} … dz dy dx $. \n",
    "\n",
    "* Reflexiv\n",
    "* Transitiv\n",
    "* Antisymmetrisch\n",
    "\n",
    "In dem Fall, dass ein Zyklus entsteht, wie z.B. $\\int_{\\alpha(y)}^{\\beta(y)} \\int_{\\rho(x)}^{\\sigma(x)} … dy dx$, ist es nicht mehr antisymmetrisch, und deshalb ist der vorliegende Term keine Funktion mehr, weil die Funktionen als eindeutige Zuordnungen definiert sind. \n",
    "\n",
    "Dann muss man alle partiellen Ordnungen in der Integration suchen und die Integration als Summe aller Integrationspfade aufschreiben. \n",
    "\n",
    "wie z.B.\n",
    "\n",
    "$\\int \\int_{\\text{Einheitskreis}} … dy dx = \\int_{-1}^{1} \\int_0^{\\sqrt{1 - x^2}} … dy dx + \\int_{-1}^{1} \\int_0^{\\sqrt{1 - y^2}} … dx dy = 2 * \\int_{-1}^{1} \\int_0^{\\sqrt{1 - x^2}} … dy dx$\n",
    "\n",
    "So werden alle Pfade einzeln integriert. Das ergibt im schlimmsten Fall $\\alpha_i(x_{\\ne i}), \\beta_i(x_{\\ne i})$ für alle $i$-ten Dimensionen, also alle Permutationen von n Dimensionen ergibt $n!$ Pfade.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "$p_i : \\Omega \\to [0, 1] \\forall i \\in [n]$ seien Wahrscheinlichkeitsfunktionen. \n",
    "\n",
    "$\\Bbb{E}[p_i | i \\in [n]]$ ist auch eine Wahrscheinlichkeitsfunktion, denn $\\int … \\int \\frac{1}{n} \\sum_{i = 1}^n p_i d\\vec{x} = \\frac{1}{n} \\sum_{i = 1}^n (\\int … \\int p_i d\\vec{x}) = \\frac{1}{n} * n = 1$\n",
    "\n",
    "$\\Pi_{i = 1}^n p_i$ ist auch eine Wahrscheinlichkeitsfunktion, denn $\\int … \\int \\Pi_{i = 1}^n p_i d\\vec{x} = 1 \\iff \\log \\int … \\int \\Pi_{i = 1}^n p_i d\\vec{x} = 0 \\overset{\\ge 0}{\\implies} \\int … \\int \\log \\Pi_{i = 1}^n p_i d\\vec{x} = 0 \\iff \\int … \\int \\Sigma_{i = 1}^n \\log p_i d\\vec{x} = 0 \\Sigma_{i = 1}^n \\iff \\int … \\int \\log p_i d\\vec{x} = 0 \\iff \\Sigma_{i = 1}^n 0 = 0 \\iff 0 = 0$ (Satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graph class which has the following methods:\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self, value, conditioned: bool = False):\n",
    "        self.value = value\n",
    "        self.conditioned = conditioned\n",
    "\n",
    "    def __repr__(self):\n",
    "        value = str(self.value)\n",
    "        if self.conditioned:\n",
    "            value += '[*]'\n",
    "        return value\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.edges = {}\n",
    "\n",
    "    def add_node(self, *args, **kwargs):\n",
    "        v = Vertex(*args, **kwargs)\n",
    "        self.edges[v] = []\n",
    "        return v\n",
    "\n",
    "    def add_edge(self, start_vertex, end_vertex):\n",
    "        if start_vertex not in self.edges:\n",
    "            raise KeyError('Start Vertex not in Graph')\n",
    "        if end_vertex not in self.edges:\n",
    "            raise KeyError('End Vertex not in Graph')\n",
    "\n",
    "        self.edges[start_vertex].append((end_vertex, '->'))\n",
    "        self.edges[end_vertex].append((start_vertex, '<-'))\n",
    "\n",
    "    def get_nodes(self):\n",
    "        return self.edges.keys()\n",
    "\n",
    "    def get_neighbors(self, vertex):\n",
    "        return self.edges[vertex]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.edges)\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = ''\n",
    "        for vertex in self.edges.keys():\n",
    "            output += vertex.value\n",
    "            output += ' -> '\n",
    "            output += str(self.edges[vertex])\n",
    "            output += '\\n'\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "\n",
    "a = graph.add_node('A')\n",
    "b = graph.add_node('B')\n",
    "c = graph.add_node('C')\n",
    "d = graph.add_node('D')\n",
    "e = graph.add_node('E')\n",
    "f = graph.add_node('F')\n",
    "g = graph.add_node('G')\n",
    "h = graph.add_node('H')\n",
    "\n",
    "graph.add_edge(a, b)\n",
    "graph.add_edge(b, c)\n",
    "graph.add_edge(d, c)\n",
    "graph.add_edge(d, e)\n",
    "graph.add_edge(f, e)\n",
    "graph.add_edge(e, g)\n",
    "graph.add_edge(f, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAnalyser:\n",
    "    def __init__(self, graph: Graph):\n",
    "        self.graph = graph\n",
    "        \n",
    "    def find_all_triples(self):\n",
    "        triples = set()\n",
    "        triples_list = []\n",
    "        for x in self.graph.get_nodes():\n",
    "            for (y, arrow1) in self.graph.get_neighbors(x):\n",
    "                for (z, arrow2) in self.graph.get_neighbors(y):\n",
    "                    arrow1_reverse = '<-' if arrow1 == '->' else '->'\n",
    "                    arrow2_reverse = '<-' if arrow2 == '->' else '->'\n",
    "                    \n",
    "                    type_of_triple = None\n",
    "                    if arrow1 == '<-' and arrow2 == '<-':\n",
    "                        continue\n",
    "                    elif arrow1 == '->' and arrow2 == '->':\n",
    "                        type_of_triple = 'chain'\n",
    "                    elif arrow1 == '<-' and arrow2 == '->':\n",
    "                        type_of_triple = 'fork'\n",
    "                    else:\n",
    "                        type_of_triple = 'collider'\n",
    "                    \n",
    "                    triple = ' '.join([str(x), arrow1, str(y), arrow2, str(z)])\n",
    "                    reverse_triple = ' '.join([str(z), arrow2_reverse, str(y), arrow1_reverse, str(x)])\n",
    "                    \n",
    "                    if z != x:\n",
    "                        if triple not in triples and reverse_triple not in triples:\n",
    "                            triples.add(triple)\n",
    "                            triples_list.append((triple, type_of_triple))\n",
    "        return triples_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphAnalyser(graph).find_all_triples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
