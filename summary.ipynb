{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory and Graph Models\n",
    "\n",
    "Observational: $p((x_i)_i) = \\Pi_i p(x_i | PA_i)$ where $X_i \\perp\\!\\!\\!\\perp (X_{\\neq i} \\backslash PA_i) | PA_i$\n",
    "\n",
    "Different choice of Markovian parents results in different causal models => Quest for common separation relationships leads to $X \\bowtie Y | Z$\n",
    "\n",
    "Open -> Close: \n",
    "\n",
    "> In usual case, X and Y are correlated. If Z is intervened, they become disconnected. \n",
    "\n",
    "> For example $g(y := f(x)) = z$ or $f(y) = (x, z)$, and fixing $y$ to a constant means fixing $x$ and $z$ as well.  \n",
    "\n",
    "* Chain: $X \\to Z \\to Y$. \n",
    "* Fork: $Z \\to X, Y$\n",
    "\n",
    "Close -> Open: \n",
    "\n",
    "> X and Y have no common cause. If Z is intervened, the relationship of X and Y becomes dependent. \n",
    "\n",
    "> Take, for example $f(x, y) = z$. Fix $z$ to some constant, then you get the level set $N_{z = k} = \\{(x, y) | f(x, y) = z\\}$ where $x$ and $y$ have to depend on each other. \n",
    "\n",
    "* Collider (or n-th Desc): $X, Y \\to_n Z$\n",
    "\n",
    "Remember that the descendant relationship can go as far as possible, since $p(X_i | PA_i) \\ne 0$.\n",
    "\n",
    "D-separation is stricter than conditional independence ($X \\bowtie Y | Z \\implies X \\perp\\!\\!\\!\\perp Y | Z$)\n",
    "\n",
    "For any $X, Y$: D-separated if $\\forall Z . (matches(X, Y \\to_n Z) \\leftrightarrow Z \\notin S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCM\n",
    "\n",
    "$X_i := f_i(PA_i, \\eta_i) \\forall i$ with $PA_i \\subseteq X \\backslash X_i$\n",
    "\n",
    "* Observational distribution generated by $C$: $p_C((X_i)_i) = \\Pi_i p_C(X_i | PA_i)$ (**Observed**)\n",
    "* Interventional distribution generated by $C$: $p_C((X_i)_i | do(X_j := k_j)) = \\frac{p_C((X_i)_i)}{p_C(X_j | PA_j)} \\delta(X_j - k_j)$ if $p_C(X_j | PA_j) \\ne 0$ (**Unobserved**)\n",
    "\n",
    "Ladders of Causation:\n",
    "\n",
    "1. Association (Seeing): $p(Y | X)$, requires $p(\\cdot)$\n",
    "2. Intervention (Doing): $p(Y | do(X := x))$, requires $p(\\cdot)$ plus ${\\cal G}({\\cal C})$\n",
    "3. Counterfactuals (Thinking): $p(Y_X | Y'_{X'})$, requires $\\cal C$ ($\\implies {\\cal G}({\\cal C}), p(\\cdot)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Discovery\n",
    "\n",
    "* Constraint-based: Identify conditional independencies using statistical tests\n",
    "* Score-based: ML approach, minimize loss over predicting the observed data\n",
    "* SCM-based: Predict cause-effect relationships by analysing the noises\n",
    "\n",
    "Constraint-based\n",
    "\n",
    "Causal Markov Condition: ''d-separation implies independence'' (see Bayesian networks)\n",
    "\n",
    "1. Data Generation by SCM\n",
    "2. Acyclicity\n",
    "3. Causal Faithfulness: ''independence implies d-separation''\n",
    "4. Causal Sufficiency\n",
    "\n",
    "1-3: ''independence implies d-separation''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence Testing\n",
    "\n",
    "Hypothesis testing (frequentist): \n",
    "* Assume iid samples of random variables $X_i \\sim p_{X; \\theta}$\n",
    "* $p_{X; \\theta}$ depends on $\\theta$ with unknown value\n",
    "\n",
    "* Define a hypothesis $H_0: \\theta = \\theta_{0}$ for some $\\theta_{0}$\n",
    "* Choose test statistic $T$\n",
    "* Obtain null distribution $p(T | H_0)$\n",
    "* Fix a significance level $\\alpha \\in ]0, 1[$ for determining the rejection threshold\n",
    "* Collect relevant data $D_n$ consisting of n samples and calculate $T(data)$\n",
    "* Reject $H_0$ if $T(D_n)$ is outside region (or if p-value $\\le \\alpha$)\n",
    "\n",
    "Given power: $TN = 1 - \\beta$, high power depends on:\n",
    "* Effect size (dependency strength)\n",
    "* Significance $\\alpha$-level\n",
    "* [DoF] Degrees of freedom $\\sim$ sample size - number of parameters to estimate\n",
    "\n",
    "Given $m > 1$ tests:\n",
    "* $P(\\bigvee_i \\alpha_i) = 1 - P(\\bigwedge_i \\neg \\alpha_i) \\le 1 - P(\\bigwedge \\neg \\frac{\\alpha}{m}) = P(\\bigvee \\frac{\\alpha}{m}) = m * \\frac{\\alpha}{m} = \\alpha$\n",
    "\n",
    "Correlation Coefficients: \n",
    "\n",
    "* Pearson Correlation Coefficient:\n",
    "    * $\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X) Var(Y)}}$ for $(X_i, Y_i) \\overset{iid}{\\sim} p_{X, Y}$\n",
    "    * $X \\perp\\!\\!\\!\\perp Y \\implies \\rho(X, Y) = 0$ (or $\\iff$ if $X, Y$ jointly Gaussian)\n",
    "    * Estimate as:\n",
    "        * $\\hat{\\rho}(X, Y) = \\frac{\\hat{Cov}(X, Y)}{\\sqrt{\\hat{Var}(X) \\hat{Var}(Y)}}$\n",
    "        * $\\hat{Cov}(X, Y) = \\frac{1}{n}\\sum_{i = 1}^n (X_i - \\overline{X_n})(Y_i - \\overline{Y_n})$\n",
    "        * $\\hat{Var}(X) = \\frac{1}{n}\\sum_{i = 1}^n (X_i - \\overline{X_n})^2$\n",
    "        * $\\hat{Var}(Y) = \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\overline{Y_n})^2$\n",
    "\n",
    "* Distance Correlation Coefficient:\n",
    "    * $dCor(X, Y) = \\frac{dCov(X, Y)}{\\sqrt{dVar(X) dVar(Y)}}$ for $(X_i, Y_i) \\overset{iid}{\\sim} p_{X, Y}$\n",
    "    <!-- * $X \\perp\\!\\!\\!\\perp Y \\implies dCor(X, Y) = 0$ -->\n",
    "    * Estimate as:\n",
    "        * $dCor(X, Y) = \\frac{dCov(X, Y)}{\\sqrt{dVar(X) dVar(Y)}}$\n",
    "        * $dCov(r_X, r_Y) = \\frac{1}{n} \\sqrt{\\sum_{i, j} A_{ij} \\cdot B_{ij}}$\n",
    "        * $dVar(r_X) = \\frac{1}{n} \\sqrt{ \\sum_{i, j} A_{ij}^2}$\n",
    "        * $dVar(r_Y) = \\frac{1}{n} \\sqrt{ \\sum_{i, j} B_{ij}^2}$\n",
    "\n",
    "Linear Dependency Tests with Additive Noise: \n",
    "\n",
    "* Multivariate Gaussian:\n",
    "    * Partial Correlation Test (ParCorr)\n",
    "    * Null hypothesis: $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $X, Y, Z$ jointly Gaussian and $\\#(Z) = p$\n",
    "    * Assume $X = Z \\beta_X + \\epsilon_X, Y = Z \\beta_Y + \\epsilon_Y$ for all known common causes $Z$\n",
    "    * Predict $\\hat{\\beta}_X, \\hat{\\beta}_Y$ by minimizing OLS\n",
    "    * Residuals $r_X = X - Z \\hat{\\beta}_X, r_Y = Y - Z \\hat{\\beta}_Y$ \n",
    "    * Residuals are uncorrelated, also $\\rho(r_X, r_Y) = 0$ if $X \\perp\\!\\!\\!\\perp Y | Z$\n",
    "    * $t$-Test statistic: $T = \\sqrt{n - p - 2} \\frac{\\hat{\\rho}(r_X, r_Y)}{\\sqrt{1 - \\hat{\\rho}(r_X, r_Y)^2}}$ with $p$ as cardinality of $Z$\n",
    "    * $T \\sim T_{n - p - 2}$\n",
    "\n",
    "    * If $p = 0$: \n",
    "        * $r_X = \\epsilon_X + Z (\\beta_X - \\hat{\\beta}_X), r_Y = \\epsilon_Y + Z (\\beta_Y - \\hat{\\beta}_Y)$\n",
    "        * If $Z = 0$, residuals are $r_X = X, r_Y = Y$\n",
    "\n",
    "* Discrete:\n",
    "    * Null hypothesis: $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $X, Y, Z$ discrete and $\\#(Z) = p$\n",
    "    * $X \\perp\\!\\!\\!\\perp Y | Z \\iff p(X, Y | Z) = p(X | Z) p(Y | Z)$\n",
    "    * $\\Bbb{E}[S_{XYZ}^{abc} | S_{XZ}^{ac} = s_{XZ}^{ac}, S_{YZ}^{bc} = s_{YZ}^{bc}] = \\frac{s_{XZ}^{ac} \\cdot s_{YZ}^{bc}}{s_Z^{c}}$\n",
    "    * $\\chi^2$-Test statistic: $G^2 = 2\\sum_{abc} s_{XYZ}^{abc} \\log (\\frac{s_{XYZ}^{abc} \\cdot s_Z^c}{s_{XZ}^{ac} \\cdot s_{YZ}^{bc}})$\n",
    "    * $G^2 \\overset{n \\to \\infty}{\\sim} \\chi_v^2$ with DoF: $(m_X - 1)(m_Y - 1)\\max(p, 1)$\n",
    "    \n",
    "    * If $p = 0$: \n",
    "        * Replace $s_Z^c = 1$ and $\\sum_{abc} … = n \\sum_{ab}$\n",
    "        * Then $G^2 = 2\\sum_{ab} s_{XY}^{ab} \\log (\\frac{s_{XY}^{ab} \\cdot n}{s_{X}^{a} \\cdot s_{Y}^{b}})$\n",
    "        * DoF: $(m_X - 1)(m_Y - 1)$\n",
    "\n",
    "Non-linear Dependency Tests with Additive Noise:\n",
    "\n",
    "* Multivariate Gaussian:\n",
    "    * Gaussian process distance correlation test (GPDC)\n",
    "    * Null hypothesis $H_0: X \\perp\\!\\!\\!\\perp Y | Z$ and $\\#(Z) = p$\n",
    "    * Assume $X = f_X(Z) + \\epsilon_X, Y = f_Y(Z) + \\epsilon_Y$ for all known common causes $Z$\n",
    "    * Residuals $r_X = X - \\hat{f_X}(Z), r_Y = Y - \\hat{f_Y}(Z)$\n",
    "    * $dCor(r_X, r_Y) = \\frac{dCov(r_X, r_Y)}{\\sqrt{dVar(r_X) dVar(r_Y)}}$\n",
    "    * Theoretical test distribution not known, use permutation testing for $dCor(r_X, r_Y)$\n",
    "\n",
    "Conditional Mutual Information as general, non-parametric dependence measure:\n",
    "* CMI-kNN: $\\hat{I}(X; Y | Z) = \\psi(k) + \\frac{1}{n} \\sum_{i = 1}^n [\\psi(k_i^z) - \\psi(k_i^{xz}) - \\psi(k_i^{yz})]$\n",
    "* Use the local nearest neighbor permutation scheme introduced for performing a permutation test\n",
    "\n",
    "Discussion:\n",
    "\n",
    "* More general tests with less strict assumptions tend to be computationally more expensive, have smaller power for the same sample size, and are statistically hard.\n",
    "* CI-testing: Keep $X - Y$, if $H_0$ (independence assumption) is rejected\n",
    "* Higher power -> Less probability of assuming independence when it is not true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "Features of Algorithms:\n",
    "\n",
    "* Soundness: All found solutions are correct\n",
    "* Completeness: All correct solutions are found\n",
    "\n",
    "DAG enumeration algorithm:\n",
    "\n",
    "1. $G = \\{Gi | \\text{Gi is a DAG with vertices X}\\}$\n",
    "1. $G' = \\{Gi | (X \\bowtie Y | Z)_{Gi} \\leftrightarrow (X \\perp\\!\\!\\!\\perp Y | Z)\\}$\n",
    "1. Orient edges in any skeleton $\\hat{G}$ shared by all DAGs in $G'$\n",
    "\n",
    "* Criticism: Infeasible to iterate over all DAGs\n",
    "\n",
    "SGS algorithm:\n",
    "\n",
    "Idea: \n",
    "\n",
    "* If $\\exists S$ s.t. $X \\perp\\!\\!\\!\\perp Y | S$: \n",
    "    * $Z \\notin S \\iff X \\to Z \\leftarrow Y$\n",
    "    * All triples between $X$ and $Y$ should be blocked\n",
    "* Else: $X - Y$\n",
    "\n",
    "1. $\\hat{G}$ as fully connected unoriented graph\n",
    "1. $\\forall X - Y$ do\n",
    "1. $\\exists S \\subset X \\backslash \\{X, Y\\} . X \\perp\\!\\!\\!\\perp Y | S \\implies$\n",
    "    1. Remove edge $X - Y$ from $\\hat{G}$\n",
    "    1. $S$ = Sepset($X - Y$) = Sepset($Y - X$)\n",
    "1. $\\forall X - Z - Y$ do\n",
    "1. $Z \\notin$ Sepset($X - Y$) $\\implies$\n",
    "    1. $X \\to Z \\leftarrow Y$\n",
    "1. Orient while applies:\n",
    "    1. $\\forall X \\to Z - Y$ do \n",
    "        * $X \\to Z \\to Y$\n",
    "    1. $\\forall X \\to Z \\to Y - X$ do \n",
    "        * $X \\to Z \\to Y \\leftarrow X$\n",
    "    1. $\\forall (X - Y, W, Z$ and $Y, Z \\to W)$ do\n",
    "        * $X - Y, Z$ and $X, Y, Z \\to W$\n",
    "\n",
    "* Criticism: Part 1 is still infeasible\n",
    "\n",
    "PC algorithm:\n",
    "\n",
    "Idea: Remove $X - Y$ iff $\\exists S \\subset T_{XY} \\subset X \\backslash \\{X, Y\\}$ s.t. $X \\perp\\!\\!\\!\\perp Y | S$\n",
    "\n",
    "* $Y \\leftrightarrow_n X$ excluded due to acyclicity\n",
    "* If $Y \\not\\to_n X$: $T_{XY} := pa(G, Y)$\n",
    "* If $X \\not\\to_n Y$: $T_{XY} := pa(G, X)$\n",
    "\n",
    "But: unknown parents\n",
    "\n",
    "* $T_X \\supset pa(G, X), T_Y \\supset pa(G, Y)$ \n",
    "* No $X - Y$ iff $\\exists S \\subset T_X \\lor S \\subset T_Y . (X \\perp\\!\\!\\!\\perp Y | S)$\n",
    "\n",
    "1. $\\hat{G}$ as fully connected unoriented graph\n",
    "1. $p := 0$\n",
    "1. while $\\exists X \\to Y . |adj(\\hat{G}, Y) \\backslash \\{X\\}| \\ge p$ do\n",
    "1. $\\forall X \\to Y$ \n",
    "1. $\\exists S \\subset adj(\\hat{G}, Y) \\backslash \\{X\\} . (|S| = p, X \\perp\\!\\!\\!\\perp Y | S) \\implies$\n",
    "    1. Remove edge $X \\to Y$ from $\\hat{G}$\n",
    "    1. $S$ = Sepset($X \\to Y$) = Sepset($Y \\to X$)\n",
    "1. $p := p + 1$\n",
    "\n",
    "Extensions, Relaxing assumptions:\n",
    "\n",
    "Independence tests:\n",
    "* Unknown ground-truth independencies\n",
    "* Thus, $X \\perp\\!\\!\\!\\perp Y | S$ to $CI-Test(X, Y, S)$\n",
    "* No guarantee to find true CPDAG\n",
    "* Still converges for $n \\to \\infty$ samples to true CPDAG\n",
    "\n",
    "Order independence:\n",
    "* Output of PC algorithm may depend on the order of input variables\n",
    "* Thus, PC-stable algorithm\n",
    "\n",
    "Weaker faithfulness:\n",
    "* Strong assumption\n",
    "* Thus, conservative PC algorithm requires weaker adjacency faithfulness\n",
    "\n",
    "Causal Insufficiency:\n",
    "* Unobserved variables\n",
    "* Thus, FCI algorithm includes latent / selection variables\n",
    "\n",
    "Cyclicity:\n",
    "* Applicability of PC algorithm\n",
    "* Still, P remains sound and complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Discovery via Restricted SCMs\n",
    "\n",
    "Previously:\n",
    "* D := iid sample from P(X)\n",
    "* I(D) := independence tests\n",
    "* G(I) := faithfulness markov \n",
    "\n",
    "Now:\n",
    "* D := iid sample from P(X)\n",
    "* R(D) := fit restricted SCM\n",
    "* G(R) := read off causal graph\n",
    "\n",
    "Constraint-based causal discovery can only determine Markov eq.class \n",
    "\n",
    "Non-uniqueness of graph structures:\n",
    "\n",
    "$\\forall p(x, y) . \\exists SCM . X := \\eta_X, Y := f_Y(X, \\eta_Y)$ with measurable $f_Y$ and $\\eta_X \\perp\\!\\!\\!\\perp \\eta_Y$\n",
    "\n",
    "Proof for all $p(x, y)$:\n",
    "* Select $f_Y(x, \\eta_Y) := F_{Y | X}^{-1}(\\eta_Y)$\n",
    "* $F_{Y | x}^{-1}(y) = P(Y \\le y | X = x)$ cond. cum. dist.\n",
    "* Draw $\\eta_Y$ uniformly and independent of $X$\n",
    "\n",
    "Linear Additive Models\n",
    "* $p(X, Y)$ admits a linear additive noise model from X to Y if generated by a SCM, s.t. $X := \\eta_X$ and $Y := \\alpha X + \\eta_Y$ with $\\eta_X \\perp\\!\\!\\!\\perp \\eta_Y$ \n",
    "* Additionally, $p(X, Y)$ admits from Y to X iff $X$ and $\\eta_Y$ are Gaussian \n",
    "    * If this is the case, non-identifiable in which direction\n",
    "\n",
    "Non-identifiability:\n",
    "* Let $\\perp\\!\\!\\!\\perp (X_1, …, X_n)$\n",
    "* Let $a_j, b_j \\ne 0$ s.t. \n",
    "* If $(I_1 := \\sum_j a_jX_j) \\perp\\!\\!\\!\\perp (I_2 := \\sum_j b_jX_j)$\n",
    "* Then all $X_j$ are Gaussian\n",
    "* Further assume $Y = \\alpha X + \\eta_Y$ and $X = \\beta Y + \\eta_X$\n",
    "* With $Y \\perp\\!\\!\\!\\perp \\eta_X$ and Gaussian $X \\perp\\!\\!\\!\\perp \\eta_Y$ \n",
    "* Then $I_1 := Y = \\alpha X + \\eta_Y$ is Gaussian \n",
    "* And $I_2 := \\eta_X = (1 - \\alpha \\beta) X - \\beta \\eta_Y$ is Gaussian\n",
    "\n",
    "LiNGAM:\n",
    "* Assume $p(x, y)$ admits a linear non-gaussian additive noise $X - Y$\n",
    "* Linearly regress $X$ on $Y$ (and $Y$ on $X$)\n",
    "* Predictors: $\\hat{Y}(X)$ (and $\\hat{X}(Y)$)\n",
    "* Residuals: $r_Y = Y - \\hat{Y}(X)$ (and $r_X = X - \\hat{X}(Y)$)\n",
    "* Test if $r_Y \\perp\\!\\!\\!\\perp X$ (and $r_X \\perp\\!\\!\\!\\perp Y$)\n",
    "* If $r_Y \\perp\\!\\!\\!\\perp X$ but $r_X \\not\\perp\\!\\!\\!\\perp Y$: $X \\to Y$\n",
    "\n",
    "Principle of Independent Mechanisms:\n",
    "1. The functional assignments $X_i := f(PA_i, \\eta_i)$ of individual variables correspond to autonomous physical mechanisms\n",
    "1. Say $X \\to Y$. Then $p_X$ and $p_{Y | X}$ should contain no information about each other\n",
    "1. Assume there are $l$ many different $p_X$ and $k$ many different $p_{Y | X}$ s.t. there are $kl$ many different $p_{X, Y}$\n",
    "1. Based on (2), there are $kl$ many $p_{Y}$ and $p_{X | Y}$\n",
    "1. This means $p_Y$ and $p_{X | Y}$ (a-causal factorization) are more complicated than $p_X$ and $p_{Y | X}$ (causal factorization)\n",
    "1. We cannot apply the same argument with X and Y swapped because when $X \\to Y$, we do not assume $p_Y$ and $p_{X | Y}$ do not contain information about each other\n",
    "\n",
    "Nonlinear Additive Models\n",
    "\n",
    "* $p(X, Y)$ admits a non-linear additive noise model from X to Y if generated by a SCM, s.t. $X := \\eta_X$ and $Y := f_y(X) + \\eta_Y$ with $\\eta_X \\perp\\!\\!\\!\\perp \\eta_Y$\n",
    "\n",
    "Non-identifiability:\n",
    "* Assume that $p(x, y)$ admits an additive noise model from X to Y\n",
    "* And assume $\\forall x \\in X_\\N . \\exists y . (\\log''(p_{\\eta_Y}) (y - f_y(x)) f_y'(x) \\ne 0)$, also $f_y$ is non-linear\n",
    "* Then $p(x, y)$ also admits from Y to X iff $p(x, y)$ obeys certain non-generic constraints\n",
    "\n",
    "NonLiNGAM:\n",
    "* Assume $p(x, y)$ admits a non-linear additive noise $X - Y$\n",
    "* Non-linearly regress $X$ on $Y$ (and $Y$ on $X$)\n",
    "* Predictors: $\\hat{f}_Y(X)$ (and $\\hat{f}_X(Y)$)\n",
    "* Residuals: $r_Y = Y - \\hat{f}_Y(X)$ (and $r_X = X - \\hat{f}_X(Y)$)\n",
    "* Test if $r_Y \\perp\\!\\!\\!\\perp X$ (and $r_X \\perp\\!\\!\\!\\perp Y$)\n",
    "* If $r_Y \\perp\\!\\!\\!\\perp X$ but $r_X \\not\\perp\\!\\!\\!\\perp Y$: $X \\to Y$\n",
    "\n",
    "Post-Nonlinear Additive Models\n",
    "\n",
    "* $p(X, Y)$ admits a post-nonlinear additive noise model from X to Y if generated by a SCM, s.t. $X := \\eta_X$ and $Y := g_y(f_y(X) + \\eta_Y)$ with $\\eta_X \\perp\\!\\!\\!\\perp \\eta_Y$\n",
    "\n",
    "Non-identifiability:\n",
    "* Assume that $p(x, y)$ admits an post-linear model from X to Y\n",
    "* Then, $p(x, y)$ also admits from Y to X iff $p_x, f_y, g_y$ obey a certain differential equation\n",
    "* Thus, $p(x, y)$ must be non-generic\n",
    "* Nonlinearity of $f_y$ is necessary for identifiability\n",
    "\n",
    "\n",
    "| **Name** | **Model** | **Functions** | **Identifiable** |\n",
    "|-------------------|-------------------------|-------------------------|-------------------------|\n",
    "| **General SCM** | $X_j := f_j(PA_j, \\eta_j)$ | any | no |\n",
    "| **ANM** | $X_j := f_j(PA_j) + \\eta_j$ | nonlinear | yes |\n",
    "| **CAM** | $X_j := \\sum_{PA_j} f_j(X_i) + \\eta_j$ | nonlinear | yes |\n",
    "| **Linear Gaussian** | $X_j := \\sum_{PA_j} c_{ij} X_i + \\eta_j$ | linear | no |\n",
    "| **Linear non-Gaussian** | $X_j := \\sum_{PA_j} c_{ij} X_i + \\eta_j$ | linear | yes |\n",
    "| **Linear Gaussian with eq. error var** | $X_j := \\sum_{PA_j} c_{ij} X_i + \\eta_j$ | linear | yes |\n",
    "\n",
    "Information-Geometric Causal Inference (IGCI):\n",
    "\n",
    "* If $X \\sim p_X, Y := f(X)$ is invertible\n",
    "* Then $p_X$ and $f$ do not contain information about each other\n",
    "* Given $X \\in [0, 1]$, the joint distribution $p(x, y)$ admits an IGCI model \n",
    "    * if $p_X$ is smooth and continuous and $Y = f(X)$ \n",
    "    * with $f$ smooth, invertible, strictly monotonic function \n",
    "    * with $f(0) = 0$ and $f(1) = 1$, such that \n",
    "* $cov[\\log f', p_X] = \\int_0^1 \\log f'(x) p_X(x) dx - \\int_0^1 \\log f'(x) dx = 0$\n",
    "\n",
    "Non-Identifiability:\n",
    "\n",
    "* If $p(x, y)$ admits an IGCI model from X to Y:\n",
    "    * Then from Y to X as well iff $f = id$\n",
    "\n",
    "Intuition:\n",
    "\n",
    "* Given $Y, (X_1, …, X_n)$ variables\n",
    "* Given $p_e(x, y) := $ samples from environment $e$\n",
    "* If $\\exists PA_y \\subseteq \\{X_i, …, X_n\\} . \\forall e, f . (p_e(y | PA_y) = p_f(y | PA_y))$\n",
    "    * Then $p_e(y | PA_y)$ is a stable physical mechanism\n",
    "    * And $PA_y$ are causal parents of Y\n",
    "* Ex. $p_X$: altitude, $p_{Y | X}$ mean temperature given altitude\n",
    "    * $p_X$ is variable, but $p_{Y | X}$ is stable\n",
    "\n",
    "Other approaches might use online observational or interventional data to learn causal relationships using ML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent and Selection Variables\n",
    "\n",
    "Selection Bias, Latent Confounding\n",
    "\n",
    "* $X = \\{0, 1\\}$ treatment, $Y = \\{0, 1\\}$ patient recovery\n",
    "* Suppose $X \\not\\to Y$\n",
    "* But $X = 1$ has unpleasant side-effects $Z$\n",
    "* Its severity is influenced by unobserved general health level $L$\n",
    "* $L$ is the only cause of patient recovery\n",
    "* Patients suffering from side-effects drop out of study $S = 0$\n",
    "* Thus, data only contains patients with $S = 1$ conditioned\n",
    "* So, $X$ and $Y$ is falsely correlated, because collider $Z$ is opened\n",
    "\n",
    "Ancestral Graphs:\n",
    "\n",
    "* DAGs with unobserved variables would have infinite nodes\n",
    "* Don't constrain graph and topology, represent them with ancestral relationships\n",
    "* For any DAG there is a MAG\n",
    "\n",
    "MAG (Maximal Ancestral Graph):\n",
    "* Edge Types:\n",
    "    * Directed (Parent $\\to$ Child)\n",
    "    * Bidirected (Spouse $\\leftrightarrow$ Spouse): $L \\to X, Y$\n",
    "    * Undirected (Neighbor $-$ Neighbor): $X, Y \\to S$ \n",
    "* Directed cycle if $X_i \\to X_j$ and $X_j$ ancestor of $X_i$\n",
    "* Almost directed cycle if $X_i \\leftrightarrow X_j$ and $X_j$ ancestor of $X_i$\n",
    "* Collider $X_k$ if $X_i *\\to X_k \\leftarrow* X_j$\n",
    "* Ancestral graph: \n",
    "    * If no directed or almost directed cycles \n",
    "    * And $\\forall X_i - X_j$, both $X_i$ and $X_j$ have no parents or spouses\n",
    "* $X \\in An(\\{Y\\} \\cup S)$ means $X$ is a cause of $Y$ or any selection variable $S$\n",
    "* Inducing path $p$ between $X_i, X_j$ relative to $\\{L, S\\}$ with:\n",
    "    * $X_k$ is collider $\\implies X_k \\in An(\\{X_i, X_j\\} \\cup S)$\n",
    "    * $X_k$ is non-collider $\\implies X_k \\in L$\n",
    "    * If $X_i$ and $X_j$ are adjacent, trivially inducing\n",
    "* Active (m-connecting) path relative to $Z$ with $X_i, X_j \\notin Z$:\n",
    "    * $X_k$ is non-collider and $X_k \\notin Z$\n",
    "    * $X_k$ is collider and $X_k \\in An(Z)$\n",
    "* $X_i$ and $X_j$ m-separated by Z if no active path between them\n",
    "* Maximal AG: \n",
    "    * iff $\\forall X_i, X_j$ non-adjacent $\\exists Z$ m-separating them\n",
    "    * iff $\\forall X_i, X_j$ no inducing path relative to $\\{\\}$\n",
    "\n",
    "* $X_i$ and $X_j$ m-connected relative to $Z \\cup S$:\n",
    "    * iff $\\exists$ inducing path between $X_i, X_j$ rel. to $\\{L, S\\}$\n",
    "    \n",
    "DAG to MAG:\n",
    "* Add edge $X_i, X_j \\in X$ in MAG iff $\\exists$ inducing path rel. to $\\{L, S\\}$\n",
    "* Apply orientation:\n",
    "    * $X_i \\to X_j$ if $X_i \\in An(X_j \\cup S)$ and $X_j \\notin An(X_i \\cup S)$\n",
    "    * $X_i \\leftrightarrow X_j$ if $X_i \\notin An(X_j \\cup S)$ and $X_j \\notin An(X_i \\cup S)$\n",
    "    * $X_i - X_j$ if $X_i \\in An(X_j \\cup S)$ and $X_j \\in An(X_i \\cup S)$\n",
    "\n",
    "PAG:\n",
    "* Similar to CPDAG of DAGs\n",
    "* MAG with undirected edges as well, denoted as circle marks\n",
    "* Maximally informative for [M] if every circle in $P$ is a variant mark\n",
    "\n",
    "Discovery Algorithms for L, S Variables\n",
    "\n",
    "Fast Causal Inference (FCI) Algorithm:\n",
    "\n",
    "1. Skeleton Discovery (for finding initial skeleton and separating sets, => PC)\n",
    "2. Orienting Colliders (=> PC)\n",
    "3. Further Update Skeleton\n",
    "4. Further Orient Colliders\n",
    "5. Exhaustively Apply 10 Orientation Rules\n",
    "\n",
    "Order of Analysis\n",
    "1. Observed Dependence: Possible causal models?\n",
    "1. Causal Markov Condition: statistical dependence => causal connectedness\n",
    "1. Add Z and assume no unobserved variables\n",
    "1. Observed conditional independence: $X \\perp\\!\\!\\!\\perp Y | Z \\iff p(X, Y | Z) = p(X | Z)p(Y | Z)$\n",
    "1. Faithfulness Assumption: statistical independence => no causal connectedness\n",
    "1. Markov Equivalence: Cannot distinguish graphs\n",
    "1. Suppose $(V \\perp\\!\\!\\!\\perp W), (X \\perp\\!\\!\\!\\perp Y | Z), (X \\perp\\!\\!\\!\\perp V | Z), (X \\perp\\!\\!\\!\\perp W | Z), (Y \\perp\\!\\!\\!\\perp V | Z), (Y \\perp\\!\\!\\!\\perp W | Z)$\n",
    "1. Assuming no latent variables: Which causal models explain these?\n",
    "1. Only possible model: $V, W \\to Z \\to X, Y$\n",
    "1. Only possible model (accounting for latents): $V, W \\circ\\to Z \\to X, Y$\n",
    "\n",
    "Selection Bias\n",
    "\n",
    "1. For $X \\perp\\!\\!\\!\\perp Y$, observe samples: \n",
    "    1. For $S = (Y > 0)$: $Y \\to S$\n",
    "    1. For $S = (X > 0)$: $X \\to S$\n",
    "    1. For $S = (X > 0) \\lor (Y > 0)$: $X, Y \\to S$ \n",
    "1. For $X \\to Y$, observe samples:\n",
    "    1. For $S = (Y > 0)$: $X \\to Y \\to S$\n",
    "    1. For $S = (X > 0)$: $X \\to S, Y$\n",
    "    1. For $S = (X > 0) \\lor (Y > 0)$: $S \\leftarrow X \\to Y \\to S$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n",
    "Constraint-based Causal Discovery\n",
    "\n",
    "* Multivariate time series: $X := (X_1, …, X_n)$ observed at time steps $t \\in \\Z$\n",
    "* Time series graph: One vertex per variable per time step\n",
    "* Assume stationarity\n",
    "    * Repetitive structure\n",
    "    * No need for absolute timestamps\n",
    "    * Multiple samples in a sliding window manner\n",
    "* Observe $X$ in time window $[t_0 - T, t_0]$\n",
    "* Learn the time series graph by testing CI\n",
    "* Sufficient to learn the subgraph within finite time $[t - \\tau_{\\max}, t]$\n",
    "\n",
    "Time Series Graphs\n",
    "* Lagged Edges: $X_{i, s - \\tau} \\to X_{j, s}$ with lag $\\tau \\ge 0$\n",
    "* Contemporaneous Edges: $X_{i, s} \\to X_{j, s}$\n",
    "* Autodependency Edges: $X_{i, s - \\tau} \\to X_{i, s}$\n",
    "* Process order: Maximal lag in graph\n",
    "* Maximal considered lag $\\tau_{\\max}$ \n",
    "    * Specifies the time window $[t - \\tau_{\\max}, t]$\n",
    "    * Must be at least as large as the process order\n",
    "    * Chosen through scientific domain knowledge or in a data driven way      \n",
    "\n",
    "Causal Discovery:\n",
    "* Modify an algorithm (take PC) having the following phases: \n",
    "    * Skeleton link removal phase using CI-tests:\n",
    "        * Partial Correlation\n",
    "        * Distance Correlation\n",
    "        * Conditional Mutual Information\n",
    "    * Orientation phase\n",
    "* With the following background knowledge:\n",
    "    * Repetitive structure\n",
    "    * Causation cannot go back in time\n",
    "    * Only learn edges that end at $t$\n",
    "    * Possible ambiguity of contemporaneous edges during the orientation\n",
    "* Problem:\n",
    "    * Low statistical detection power: Such tests judge independence although dependence is true\n",
    "    * Inflated false positives: Samples not iid because autocorrelated\n",
    "\n",
    "Statistical power for detecting $X \\not\\perp\\!\\!\\!\\perp Y | S$:\n",
    "1. Sample size (Given by dataset)\n",
    "2. Significance level $\\alpha$ (Given as hyperparameter by researcher)\n",
    "3. Condition dimension/complexity $|S|$ (PC optimizes this)\n",
    "4. Effect size, dependency strength\n",
    "\n",
    "Given $Z \\to X \\to Y \\leftarrow W$, then $I(X; Y | Z) \\le I(X; Y) \\le I(X; Y | W)$:\n",
    "* Proof of $I(X; Y | Z) \\le I(X; Y)$\n",
    "    * $I(X; Y | Z) \\le I(X; Y)$\n",
    "    * $ \\iff I(X; Y) + I(X; Z | Y) - I(X; Z) \\le I(X; Y)$\n",
    "    * $ \\iff I(X; Z | Y) \\le I(X; Z)$ (and $X \\perp\\!\\!\\!\\perp Z | Y$)\n",
    "    * $ \\iff 0 \\le I(X; Z)$ (true)\n",
    "* Proof of $I(X; Y | W) \\ge I(X; Y)$\n",
    "    * $I(X; Y | W) \\ge I(X; Y)$\n",
    "    * $ \\iff I(X; Y) + I(X; W | Y) - I(X; W) \\ge I(X; Y)$\n",
    "    * $ \\iff I(X; W | Y) \\ge I(X; W)$ (and $X \\perp\\!\\!\\!\\perp W$)\n",
    "    * $ \\iff I(X; W | Y) \\ge 0$ (true)\n",
    "* Conditioning on $Z$ (latent variable):\n",
    "    * Reduced effect size\n",
    "    * More judgments of independence\n",
    "    * Less $1 - \\beta$, but more $1 - \\alpha$\n",
    "* Conditioning on $W$ (selection variable):\n",
    "    * Increased effect size\n",
    "    * Less judgments of independence\n",
    "    * More $1 - \\beta$, but less $1 - \\alpha$\n",
    "\n",
    "Modify PC algorithm as:\n",
    "* Test $X \\not\\perp\\!\\!\\!\\perp Y | S$ for all $X,Y$ adjacent conditions $S$\n",
    "* Remove link if $\\min_S I(X; Y | S) < I_{\\alpha_{PC}}$\n",
    "* Previous PC algorithm: \n",
    "    * $\\exists S . I(X; Y | S) = 0$ \n",
    "    * $\\iff \\min_S I(X; Y | S) = 0$ \n",
    "    * $\\implies \\min_S I(X; Y | S) < I_{\\alpha_{PC}}$\n",
    "\n",
    "PCMCI+:\n",
    "* Choice of sepsets influence the performance at orientation phase,\n",
    "* Improve the reliability of CI tests by optimizing the choice of cond. sets\n",
    "* Phases:\n",
    "    * $PC_1$ lagged phase:\n",
    "        1. Iterate through lagged links only\n",
    "        1. Sort them by association strength with $X_{j, t}$\n",
    "        1. Select top $p$ conditions\n",
    "        \n",
    "        Result: $\\forall p . S := \\{{\\cal A}(X_{j, t})\\}_{l=1}^p$\n",
    "        * Less likely to cond. on effect-size weakening parents of $X_{j, t - \\tau}$\n",
    "        * Still sufficient to block most paths\n",
    "        * Converges to lagged parents + parents of contemporaneous ancestors $\\hat{\\beta}^{-}_t({X_{j, t}})$\n",
    "            * Also all contemporaneous ancestors are replaced by their parents\n",
    "            * No more contemporaneous links\n",
    "        \n",
    "        \n",
    "    * $MCI$ contemporaneous phase\n",
    "        * Goal: Remove spurious links due to contemporaneous drivers and store sepsets\n",
    "        * Initialize with lagged links $\\hat{\\beta}^{-}_t({X_{j, t}})$ and all contemporaneous links\n",
    "        * Conduct further CI tests for link removals\n",
    "        * $\\forall i, j, \\tau$\n",
    "            * $X_{i, t - \\tau} \\perp\\!\\!\\!\\perp X_{j, t} | S$ (block contemporaneous paths)\n",
    "            * $\\cup \\hat{\\beta}^{-}_t({X_{j, t}}) \\backslash \\{X_{i, t - \\tau}\\}$ (block lagged paths, see $X_{i, t - \\tau}$ might be a parent of $X_{j, t}$)\n",
    "            * $\\cup \\hat{\\beta}^{-}_{t - \\tau}({X_{i, t - \\tau}})$ (block lagged paths, see $X_{j, t}$ is not a parent of $X_{i, t - \\tau}$)\n",
    "        * Then the effect size larger than PC effect size\n",
    "    * Orientation phase\n",
    "\n",
    "* PCMCI+ is sound and complete\n",
    "* MCI tests are well-calibrated also for autocorrelated data\n",
    "\n",
    "Granger Causality\n",
    "\n",
    "* Let ${\\cal X}_{\\ne i, <t}$ the past of all time series but that of $X_i$\n",
    "* Let $X_{i, <t}$ the past of $X_i$\n",
    "* $X_i$ Granger cause $X_j$\n",
    "    * If $X_{j, t} \\not\\perp\\!\\!\\!\\perp X_{i, <t} | {\\cal X}_{\\ne i, <t}$\n",
    "    * If all edges are lagged, then it suffices to find $\\tau > 0$ s.t. $X_{i, t - \\tau} \\to X_{j, t}$ \n",
    "    * Or statistically, if model 2 has a significantly smaller error:\n",
    "        * Model 1: $X_{j, t} = \\sum_{\\tau = 1}^m \\sum_{k \\ne i} a_{\\tau k}X_{k, t - \\tau} + \\eta_t$\n",
    "        * Model 2: $X_{j, t} = \\sum_{\\tau = 1}^m \\sum_{k \\ne i} a_{\\tau k}X_{k, t - \\tau} + \\sum_{\\tau = 1}^m a_{\\tau i}X_{i, t - \\tau} + \\eta_t'$\n",
    "* Does not allow unobserved confounders and contemporaneous causal relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Ideas\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "|                   | **Actual Positive (P)** | **Actual Negative (N)** |\n",
    "|-------------------|-------------------------|-------------------------|\n",
    "| **Predicted Positive (P')** | True Positive (TP): $1 - \\alpha$  | False Positive (FP): $\\beta$  |\n",
    "| **Predicted Negative (N')** | False Negative (FN): $\\alpha$  | True Negative (TN): $1 - \\beta$  |\n",
    "\n",
    "* Precision: $\\frac{TP}{TP + FP}$ - given all positive predictions, which are true positive\n",
    "* Recall (Sensitivity): $\\frac{TP}{TP + FN}$ - given all positive cases, which are predicted positive\n",
    "* Specificity: $\\frac{TN}{TN + FP}$ - given all negative cases, which are predicted negative\n",
    "\n",
    "* F1-Score: Harmonic mean of Precision and Recall - $\\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} = \\frac{2}{\\frac{TP + FP}{TP} + \\frac{TP + FN}{TP}} = \\frac{2}{\\frac{2 * TP + FP + FN}{TP}} = \\frac{2 * TP}{2 * TP + FP + FN} = \\frac{2 * (1 - \\alpha)}{2 * (1 - \\alpha) + \\alpha + \\beta} = \\frac{1 - \\alpha}{1 - (\\alpha + \\beta) / 2}$\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Degrees of Freedom:\n",
    "\n",
    "* If a LES has n equations and n parameters, exactly one solution\n",
    "* If a LES has n-1 equations, (linearly) infinitely many solutions (1 free variable)\n",
    "* If n-2 equations, (quadratically) infinitely many solutions (2 free variable)\n",
    "* … \n",
    "* More degrees of freedom, more multitudes of infinity\n",
    "* In an extreme case, A NN would have millions of parameters to estimate\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Bonferroni Correction:\n",
    "\n",
    "Given $m > 1$ tests:\n",
    "* Assume $\\alpha = \\Sigma_i \\alpha_i$\n",
    "* $P(\\bigvee_i \\alpha_i) = 1 - P(\\bigwedge_i \\neg \\alpha_i) = 1 - \\Pi_i P(\\alpha_i) = 1 - \\Pi_i (1 - \\alpha_i)$\n",
    "* $\\max \\frac{\\log P(\\bigvee_i \\alpha_i)}{\\Sigma_i \\alpha_i} = \\max \\frac{\\log (1 - \\Pi_i (1 - \\alpha_i))}{\\Sigma_i \\alpha_i} = \\min \\frac{\\log (\\Pi_i (1 - \\alpha_i))}{\\Sigma_i \\alpha_i} = \\min \\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}$\n",
    "* $0 = \\frac{\\partial}{\\partial \\alpha_i}(\\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}) = \\frac{\\partial}{\\partial \\alpha_i}(\\frac{\\Sigma_i \\log(1 - \\alpha_i)}{\\Sigma_i \\alpha_i}) = \\frac{\\frac{1}{\\alpha_i - 1}(\\Sigma_j \\alpha_j) - \\Sigma_j \\log(1 - \\alpha_j)}{(\\Sigma_j \\alpha_j)^2} \\sim \\frac{1}{\\alpha_i - 1}(\\Sigma_j \\alpha_j) - \\Sigma_j \\log(1 - \\alpha_j) = \\Sigma_j \\frac{\\alpha_j}{\\alpha_i - 1} - \\log(1 - \\alpha_j) = \\Sigma_j \\frac{\\alpha_j}{1 - \\alpha_i} - \\log(\\frac{1}{1 - \\alpha_j}) \\sim \\Sigma_j \\alpha_j - (1 - \\alpha_i) \\log(\\frac{1}{1 - \\alpha_j}) = \\alpha - (1 - \\alpha_i) \\Sigma_j \\log(\\frac{1}{1 - \\alpha_j}) \\sim \\alpha - (1 - \\alpha_i) k$ for some strictly positive constant $k$.\n",
    "\n",
    "Then for all $i$: $\\alpha = (1 - \\alpha_i)k \\iff \\alpha_i = 1 - \\frac{\\alpha}{k}$\n",
    "* Also, upper bound estimation yields $\\alpha_1 = … = \\alpha_m = \\frac{\\alpha}{m}$\n",
    "* $1 - \\Pi_i (1 - \\alpha_i) = 1 - \\Pi_i (1 - \\frac{\\alpha}{m}) \\le 1 - (1 - \\frac{\\alpha}{m})^m = 1 - P(\\bigwedge \\neg \\frac{\\alpha}{m}) = P(\\bigvee \\frac{\\alpha}{m}) = m * P(\\frac{\\alpha}{m}) = m * \\frac{\\alpha}{m} = \\alpha$\n",
    "* Interestingly, if $m \\to \\infty$, then $1 - (1 - \\frac{\\alpha}{m})^m \\to 1 - e^{-\\alpha}$\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Causal Sufficiency:\n",
    "\n",
    "* $p(w) = \\int p(w, l) dl$ with $L = X \\backslash W$ latent variables.\n",
    "* Implies there is no pair of distinct observed vertices with a hidden common cause (a latent variable).\n",
    "* All needed variables are within model, so the causal model is sufficient\n",
    "* Alternatively: $p((\\eta_i)_i) = \\Pi_i p(\\eta_i)$, also the noise independence\n",
    "\n",
    "<hr/>\n",
    "\n",
    "CMI (Conditional Mutual Independence):\n",
    "\n",
    "* $\\forall x, y, z$:\n",
    "    * $p(x, y | z) \\ge p(x | z) p(y | z)$\n",
    "    * $\\frac{p(x, y | z)}{p(x | z) p(y | z)} \\ge 1$\n",
    "    * $\\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} \\ge 0$\n",
    "* $\\int_z \\int_y \\int_x p(x, y, z) \\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} dx dy dz \\ge 0$\n",
    "* $\\int_z p(z) dz \\int_y \\int_x p(x, y | z) \\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} dx dy \\ge 0$\n",
    "* $I(X; Y | Z) \\ge 0$\n",
    "\n",
    "* If independent, make $\\ge$ to $=$.\n",
    "\n",
    "* Also express $I(X; Y | Z)$ using $H$-entropy as \n",
    "    * $\\int_z p(z) dz \\int_y \\int_x p(x, y | z) \\log \\frac{p(x, y | z)}{p(x | z) p(y | z)} dx dy$\n",
    "    * $ = \\int_z p(z) dz \\int_y \\int_x p(x, y | z) \\log \\frac{p(x | y, z)}{p(x | z)} dx dy$\n",
    "    * $ = -\\int_z p(z) dz \\int_y \\int_x p(x, y | z) (\\log p(x | z) - \\log p(x | y, z)) dx dy$\n",
    "    * $ = H(X | Z) - H(X | Y, Z)$\n",
    "\n",
    "* Chain Rule:\n",
    "    * $I(X; Y) + I(X; Z | Y)$ \n",
    "    * $ = (H(X) - H(X | Y)) + (H(X | Y) - H(X | Y, Z))$\n",
    "    * $ = H(X) - H(X | Y, Z)$\n",
    "    * $ = (H(X) - H(X | Z)) + (H(X | Z) - H(X | Y, Z))$\n",
    "    * $ = I(X; Z) + I(X; Y | Z)$\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Determinism:\n",
    "\n",
    "* $X = f(Z)$, then $X$ and $Z$ would have to be merged as a single node.\n",
    "* See Proof:\n",
    "    * $l(f(Z); Y | Z) = \\int_z p(z) dz \\int_y \\int_x p(f(z), y | z) \\log \\frac{p(f(z), y | z)}{p(f(z) | z) p(y | z)} dx dy$\n",
    "    * $p(f(z) | z) = 1$, because $X := f(z)$ is always satisfied if $Z := z$ \n",
    "    * $p(f(z), y | z) = p(y | z)$, because similarly $X := f(z)$ on the left side doesn't change anything.\n",
    "    * Then $p(f(z), y | z) \\log \\frac{p(f(z), y | z)}{p(f(z) | z) p(y | z)} = p(y | z) \\log \\frac{p(y | z)}{p(y | z)} = p(y | z) \\log 1 = 0$\n",
    "* Category Theory suggests that if two variables have the exactly same relationships with all other nodes, then they are the same. \n",
    "* Regard noises of variable as the characteristics of a person, and if a person is married to another to another, they are fully codependent and may be counted as one person.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevante Ideen\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Mehrfache Integration ist ein Funktional, die eine Funktion als Input nimmt und eine neue Funktion (oder eine Konstante) zurückgibt. \n",
    "\n",
    "Die unteren und oberen Grenzen der Integration müssen partiell geordnet sein, d.h. $\\int_a^b \\int_{\\alpha(x)}^{\\beta(x)} \\int_{\\rho(x, y)}^{\\sigma(x, y)} … dz dy dx $. \n",
    "\n",
    "* Reflexiv\n",
    "* Transitiv\n",
    "* Antisymmetrisch\n",
    "\n",
    "In dem Fall, dass ein Zyklus entsteht, wie z.B. $\\int_{\\alpha(y)}^{\\beta(y)} \\int_{\\rho(x)}^{\\sigma(x)} … dy dx$, ist es nicht mehr antisymmetrisch, und deshalb ist der vorliegende Term keine Funktion mehr, weil die Funktionen als eindeutige Zuordnungen definiert sind. \n",
    "\n",
    "Dann muss man alle partiellen Ordnungen in der Integration suchen und die Integration als Summe aller Integrationspfade aufschreiben. \n",
    "\n",
    "wie z.B.\n",
    "\n",
    "$\\int \\int_{\\text{Einheitskreis}} … dy dx = \\int_{-1}^{1} \\int_0^{\\sqrt{1 - x^2}} … dy dx + \\int_{-1}^{1} \\int_0^{\\sqrt{1 - y^2}} … dx dy = 2 * \\int_{-1}^{1} \\int_0^{\\sqrt{1 - x^2}} … dy dx$\n",
    "\n",
    "So werden alle Pfade einzeln integriert. Das ergibt im schlimmsten Fall $\\alpha_i(x_{\\ne i}), \\beta_i(x_{\\ne i})$ für alle $i$-ten Dimensionen, also alle Permutationen von n Dimensionen ergibt $n!$ Pfade.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "$p_i : \\Omega \\to [0, 1] \\forall i \\in [n]$ seien Wahrscheinlichkeitsfunktionen. \n",
    "\n",
    "$\\Bbb{E}[p_i | i \\in [n]]$ ist auch eine Wahrscheinlichkeitsfunktion, denn $\\int … \\int \\frac{1}{n} \\sum_{i = 1}^n p_i d\\vec{x} = \\frac{1}{n} \\sum_{i = 1}^n (\\int … \\int p_i d\\vec{x}) = \\frac{1}{n} * n = 1$\n",
    "\n",
    "$\\Pi_{i = 1}^n p_i$ ist auch eine Wahrscheinlichkeitsfunktion, denn $\\int … \\int \\Pi_{i = 1}^n p_i d\\vec{x} = 1 \\iff \\log \\int … \\int \\Pi_{i = 1}^n p_i d\\vec{x} = 0 \\overset{\\ge 0}{\\implies} \\int … \\int \\log \\Pi_{i = 1}^n p_i d\\vec{x} = 0 \\iff \\int … \\int \\Sigma_{i = 1}^n \\log p_i d\\vec{x} = 0 \\Sigma_{i = 1}^n \\iff \\int … \\int \\log p_i d\\vec{x} = 0 \\iff \\Sigma_{i = 1}^n 0 = 0 \\iff 0 = 0$ (Satisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graph class which has the following methods:\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self, value, conditioned: bool = False):\n",
    "        self.value = value\n",
    "        self.conditioned = conditioned\n",
    "\n",
    "    def __repr__(self):\n",
    "        value = str(self.value)\n",
    "        if self.conditioned:\n",
    "            value += '[*]'\n",
    "        return value\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.edges = {}\n",
    "\n",
    "    def add_node(self, *args, **kwargs):\n",
    "        v = Vertex(*args, **kwargs)\n",
    "        self.edges[v] = []\n",
    "        return v\n",
    "\n",
    "    def add_edge(self, start_vertex, end_vertex):\n",
    "        if start_vertex not in self.edges:\n",
    "            raise KeyError('Start Vertex not in Graph')\n",
    "        if end_vertex not in self.edges:\n",
    "            raise KeyError('End Vertex not in Graph')\n",
    "\n",
    "        self.edges[start_vertex].append((end_vertex, '->'))\n",
    "        self.edges[end_vertex].append((start_vertex, '<-'))\n",
    "\n",
    "    def get_nodes(self):\n",
    "        return self.edges.keys()\n",
    "\n",
    "    def get_neighbors(self, vertex):\n",
    "        return self.edges[vertex]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.edges)\n",
    "\n",
    "    def __repr__(self):\n",
    "        output = ''\n",
    "        for vertex in self.edges.keys():\n",
    "            output += vertex.value\n",
    "            output += ' -> '\n",
    "            output += str(self.edges[vertex])\n",
    "            output += '\\n'\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "\n",
    "a = graph.add_node('A')\n",
    "b = graph.add_node('B')\n",
    "c = graph.add_node('C')\n",
    "d = graph.add_node('D')\n",
    "e = graph.add_node('E')\n",
    "f = graph.add_node('F')\n",
    "g = graph.add_node('G')\n",
    "h = graph.add_node('H')\n",
    "\n",
    "graph.add_edge(a, b)\n",
    "graph.add_edge(b, c)\n",
    "graph.add_edge(d, c)\n",
    "graph.add_edge(d, e)\n",
    "graph.add_edge(f, e)\n",
    "graph.add_edge(e, g)\n",
    "graph.add_edge(f, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAnalyser:\n",
    "    def __init__(self, graph: Graph):\n",
    "        self.graph = graph\n",
    "        \n",
    "    def find_all_triples(self):\n",
    "        triples = set()\n",
    "        triples_list = []\n",
    "        for x in self.graph.get_nodes():\n",
    "            for (y, arrow1) in self.graph.get_neighbors(x):\n",
    "                for (z, arrow2) in self.graph.get_neighbors(y):\n",
    "                    arrow1_reverse = '<-' if arrow1 == '->' else '->'\n",
    "                    arrow2_reverse = '<-' if arrow2 == '->' else '->'\n",
    "                    \n",
    "                    type_of_triple = None\n",
    "                    if arrow1 == '<-' and arrow2 == '<-':\n",
    "                        continue\n",
    "                    elif arrow1 == '->' and arrow2 == '->':\n",
    "                        type_of_triple = 'chain'\n",
    "                    elif arrow1 == '<-' and arrow2 == '->':\n",
    "                        type_of_triple = 'fork'\n",
    "                    else:\n",
    "                        type_of_triple = 'collider'\n",
    "                    \n",
    "                    triple = ' '.join([str(x), arrow1, str(y), arrow2, str(z)])\n",
    "                    reverse_triple = ' '.join([str(z), arrow2_reverse, str(y), arrow1_reverse, str(x)])\n",
    "                    \n",
    "                    if z != x:\n",
    "                        if triple not in triples and reverse_triple not in triples:\n",
    "                            triples.add(triple)\n",
    "                            triples_list.append((triple, type_of_triple))\n",
    "        return triples_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphAnalyser(graph).find_all_triples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
